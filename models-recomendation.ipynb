{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273f8547",
   "metadata": {},
   "source": [
    "# Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ea4619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Title             1000 non-null   object \n",
      " 1   Description       1000 non-null   object \n",
      " 2   Price             1000 non-null   float64\n",
      " 3   Availability      1000 non-null   object \n",
      " 4   Image_URL         1000 non-null   object \n",
      " 5   Rating            1000 non-null   int64  \n",
      " 6   availability_num  1000 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 54.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>availability_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.07035</td>\n",
       "      <td>2.923000</td>\n",
       "      <td>8.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.44669</td>\n",
       "      <td>1.434967</td>\n",
       "      <td>5.654622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.10750</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.98000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.45750</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.99000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Price       Rating  availability_num\n",
       "count  1000.00000  1000.000000       1000.000000\n",
       "mean     35.07035     2.923000          8.585000\n",
       "std      14.44669     1.434967          5.654622\n",
       "min      10.00000     1.000000          1.000000\n",
       "25%      22.10750     2.000000          3.000000\n",
       "50%      35.98000     3.000000          7.000000\n",
       "75%      47.45750     4.000000         14.000000\n",
       "max      59.99000     5.000000         22.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le jeu de donn√©es\n",
    "df = pd.read_csv(\"/dataset/livres_bruts.csv\")\n",
    "\n",
    "# Aper√ßu de la structure\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec4a82",
   "metadata": {},
   "source": [
    "1000 lignes ‚Üí donc 1000 livres dans ton dataset.\n",
    "7 colonnes :\n",
    "\n",
    "- Title (str) ‚Üí titre du livre\n",
    "- Description (str) ‚Üí r√©sum√©\n",
    "- Price (float64) ‚Üí prix du livre\n",
    "- Availability (str) ‚Üí texte indiquant si le livre est en stock\n",
    "- Image_URL (str) ‚Üí lien de l‚Äôimage de couverture\n",
    "- Rating (int64) ‚Üí note (de 1 √† 5)\n",
    "- availability_num (int64) ‚Üí nombre d‚Äôexemplaires disponibles\n",
    "\n",
    "aucune donn√©e manquante dans ce dataset\n",
    "\n",
    "üìå Price\n",
    "- 25 % des livres co√ªtent ‚â§ 22.11 ¬£\n",
    "- 25 % des livres co√ªtent ‚â• 47.46 ¬£\n",
    "\n",
    "üìå Rating\n",
    "- 25 % des livres ont ‚â§ 2 √©toiles --> 25%\n",
    "- 25 % des livres ont ‚â• 4 √©toiles --> 75%\n",
    "\n",
    "üìå availability_num\n",
    "- 25 % des livres ont ‚â§ 3 exemplaires --> 25%\n",
    "- 25 % des livres ont ‚â• 14 exemplaires --> 75%\n",
    "\n",
    "**Dataset complet** ‚Üí pas de valeurs manquantes.<br>\n",
    "**Prix** : distribution relativement large (10 ¬£ √† 60 ¬£).<br>\n",
    "**Notes** : centr√©es autour de 3 √©toiles, avec une bonne proportion de 4 et 5.<br>\n",
    "**Stock** : tr√®s variable, certains livres en quantit√© limit√©e (1 ou 2), d‚Äôautres en grande quantit√© (jusqu‚Äô√† 22)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a954881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Image_URL</th>\n",
       "      <th>Rating</th>\n",
       "      <th>availability_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>Its hard to imagine a world without A Light in...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>In stock (22 available)</td>\n",
       "      <td>https://books.toscrape.com/media/cache/fe/72/f...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>Erotic and absorbing...Written with starling p...</td>\n",
       "      <td>53.74</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>https://books.toscrape.com/media/cache/08/e9/0...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>Dans une France assez proche de la n√¥tre, un h...</td>\n",
       "      <td>50.10</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>https://books.toscrape.com/media/cache/ee/cf/e...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>47.82</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>https://books.toscrape.com/media/cache/c0/59/c...</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>In stock (20 available)</td>\n",
       "      <td>https://books.toscrape.com/media/cache/ce/5f/c...</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title  \\\n",
       "0                   A Light in the Attic   \n",
       "1                     Tipping the Velvet   \n",
       "2                             Soumission   \n",
       "3                          Sharp Objects   \n",
       "4  Sapiens: A Brief History of Humankind   \n",
       "\n",
       "                                         Description  Price  \\\n",
       "0  Its hard to imagine a world without A Light in...  51.77   \n",
       "1  Erotic and absorbing...Written with starling p...  53.74   \n",
       "2  Dans une France assez proche de la n√¥tre, un h...  50.10   \n",
       "3  WICKED above her hipbone, GIRL across her hear...  47.82   \n",
       "4  From a renowned historian comes a groundbreaki...  54.23   \n",
       "\n",
       "              Availability                                          Image_URL  \\\n",
       "0  In stock (22 available)  https://books.toscrape.com/media/cache/fe/72/f...   \n",
       "1  In stock (20 available)  https://books.toscrape.com/media/cache/08/e9/0...   \n",
       "2  In stock (20 available)  https://books.toscrape.com/media/cache/ee/cf/e...   \n",
       "3  In stock (20 available)  https://books.toscrape.com/media/cache/c0/59/c...   \n",
       "4  In stock (20 available)  https://books.toscrape.com/media/cache/ce/5f/c...   \n",
       "\n",
       "   Rating  availability_num  \n",
       "0       3                22  \n",
       "1       1                20  \n",
       "2       1                20  \n",
       "3       4                20  \n",
       "4       5                20  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ceb5e2",
   "metadata": {},
   "source": [
    "# Pr√©paration du mod√®le de recommandation (similarit√© cosinus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2c087",
   "metadata": {},
   "source": [
    "Charger les descriptions nettoy√©es depuis la base de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c94314d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Its hard to imagine a world without A Light in...\n",
       "1      Erotic and absorbing...Written with starling p...\n",
       "2      Dans une France assez proche de la n√¥tre, un h...\n",
       "3      WICKED above her hipbone, GIRL across her hear...\n",
       "4      From a renowned historian comes a groundbreaki...\n",
       "                             ...                        \n",
       "995                                   Pas de description\n",
       "996    High school student Kei Nagai is struck dead i...\n",
       "997    In Englands Regency era, manners and elegance ...\n",
       "998    James Patterson, bestselling author of the Ale...\n",
       "999    Around the World, continent by continent, here...\n",
       "Name: Description, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Charger les descriptions nettoy√©es\n",
    "df.Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d7fd4",
   "metadata": {},
   "source": [
    "**1. Convertir tout le texte en minuscules : text.lower().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88dd2181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hard imagine world without light attic . now-c...\n",
       "1    erotic absorbing ... written starling power . ...\n",
       "2    dans une france assez proche de la n√¥tre , un ...\n",
       "3    wicked hipbone , girl across heart words like ...\n",
       "4    renowned historian comes groundbreaking narrat...\n",
       "5    patient twenty-nine.a monster roams halls soot...\n",
       "6    drawing extensive experience evaluating applic...\n",
       "7    heart , soul , karen hicks coming woman make f...\n",
       "8    readers laura hillenbrands seabiscuit unbroken...\n",
       "9    praise aracelis girmay : girmays every lossshe...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Description'] = df['Description'].str.lower()\n",
    "df['Description'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2800434",
   "metadata": {},
   "source": [
    "# Appliquer la tokenisation : nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e21f43",
   "metadata": {},
   "source": [
    "- Suppression de la ponctuation, caract√®res sp√©ciaux et chiffres\n",
    "- Tokenisation\n",
    "- Stemming\n",
    "- Vectorisation avec TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7d36b",
   "metadata": {},
   "source": [
    "**4. Supprimer les stopwords (mots vides) avec nltk.corpus.stopwords.words('english').**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b430b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     hard imagine world without light attic . now-c...\n",
       "1     erotic absorbing ... written starling power . ...\n",
       "2     france assez proche n√¥tre , homme sengage carr...\n",
       "3     wicked hipbone , girl across heart words like ...\n",
       "4     renowned historian comes groundbreaking narrat...\n",
       "5     patient twenty-nine.a monster roams halls soot...\n",
       "6     drawing extensive experience evaluating applic...\n",
       "7     heart , soul , karen hicks coming woman make f...\n",
       "8     readers laura hillenbrands seabiscuit unbroken...\n",
       "9     praise aracelis girmay : girmays every lossshe...\n",
       "10    since assault , miss annette chetwynd plagued ...\n",
       "11    book important complete collection sonnets wil...\n",
       "12    aaron ledbetters future planned since born . y...\n",
       "13    scott pilgrims life totally sweet . hes 23 yea...\n",
       "14    punks raw power rejuvenated rock , summer 1977...\n",
       "15    never-before-told story musical revolution hap...\n",
       "16    part fact , part fiction , tyehimba jesss much...\n",
       "17    andrew barger , award-winning author engineer ...\n",
       "18    libertarianism isnt winning elections ; first ...\n",
       "19    wherever go , whatever , . . . dont anything s...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_fr = set(stopwords.words('french'))\n",
    "stopwords_combined = stopwords_en.union(stopwords_fr)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text) # Tokenisation du texte\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords_combined] # Filtrer les stopwords\n",
    "    return \" \".join(filtered_tokens) # Reconstruire la phrase\n",
    "\n",
    "# Appliquer la fonction de suppression des stopwords √† la colonne 'text'\n",
    "df['Description'] = df['Description'].apply(remove_stopwords)\n",
    "\n",
    "# Afficher les premi√®res lignes pour v√©rifier le r√©sultat\n",
    "df['Description'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f5824",
   "metadata": {},
   "source": [
    "**5. Appliquer lemming ade NLTK pour r√©duire les mots √† leur racine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "079aff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')                 # Pour lemmatizer\n",
    "nltk.download('omw-1.4')                 # Dictionnaire multilingue pour WordNet\n",
    "nltk.download('punkt')                   # Tokenizer\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger requis par pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cb66f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Description_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hard imagine world without light attic . now-c...</td>\n",
       "      <td>[hard, imagine, world, without, light, attic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>erotic absorbing ... written starling power . ...</td>\n",
       "      <td>[erotic, absorb, write, starling, power, new, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>france assez proche n√¥tre , homme sengage carr...</td>\n",
       "      <td>[france, assez, proche, n√¥tre, homme, sengage,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wicked hipbone , girl across heart words like ...</td>\n",
       "      <td>[wicked, hipbone, girl, across, heart, word, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>renowned historian comes groundbreaking narrat...</td>\n",
       "      <td>[renowned, historian, come, groundbreaking, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>patient twenty-nine.a monster roams halls soot...</td>\n",
       "      <td>[patient, monster, roams, hall, soothe, hill, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drawing extensive experience evaluating applic...</td>\n",
       "      <td>[draw, extensive, experience, evaluate, applic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>heart , soul , karen hicks coming woman make f...</td>\n",
       "      <td>[heart, soul, karen, hicks, come, woman, make,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>readers laura hillenbrands seabiscuit unbroken...</td>\n",
       "      <td>[reader, laura, hillenbrands, seabiscuit, unbr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>praise aracelis girmay : girmays every lossshe...</td>\n",
       "      <td>[praise, aracelis, girmay, girmays, every, los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>since assault , miss annette chetwynd plagued ...</td>\n",
       "      <td>[since, assault, miss, annette, chetwynd, plag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>book important complete collection sonnets wil...</td>\n",
       "      <td>[book, important, complete, collection, sonnet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aaron ledbetters future planned since born . y...</td>\n",
       "      <td>[aaron, ledbetter, future, plan, since, bear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>scott pilgrims life totally sweet . hes 23 yea...</td>\n",
       "      <td>[scott, pilgrims, life, totally, sweet, he, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>punks raw power rejuvenated rock , summer 1977...</td>\n",
       "      <td>[punk, raw, power, rejuvenate, rock, summer, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>never-before-told story musical revolution hap...</td>\n",
       "      <td>[story, musical, revolution, happen, right, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>part fact , part fiction , tyehimba jesss much...</td>\n",
       "      <td>[part, fact, part, fiction, tyehimba, jesss, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>andrew barger , award-winning author engineer ...</td>\n",
       "      <td>[andrew, barger, author, engineer, extensively...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>libertarianism isnt winning elections ; first ...</td>\n",
       "      <td>[libertarianism, isnt, win, election, first, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wherever go , whatever , . . . dont anything s...</td>\n",
       "      <td>[wherever, go, whatever, dont, anything, stupi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Description  \\\n",
       "0   hard imagine world without light attic . now-c...   \n",
       "1   erotic absorbing ... written starling power . ...   \n",
       "2   france assez proche n√¥tre , homme sengage carr...   \n",
       "3   wicked hipbone , girl across heart words like ...   \n",
       "4   renowned historian comes groundbreaking narrat...   \n",
       "5   patient twenty-nine.a monster roams halls soot...   \n",
       "6   drawing extensive experience evaluating applic...   \n",
       "7   heart , soul , karen hicks coming woman make f...   \n",
       "8   readers laura hillenbrands seabiscuit unbroken...   \n",
       "9   praise aracelis girmay : girmays every lossshe...   \n",
       "10  since assault , miss annette chetwynd plagued ...   \n",
       "11  book important complete collection sonnets wil...   \n",
       "12  aaron ledbetters future planned since born . y...   \n",
       "13  scott pilgrims life totally sweet . hes 23 yea...   \n",
       "14  punks raw power rejuvenated rock , summer 1977...   \n",
       "15  never-before-told story musical revolution hap...   \n",
       "16  part fact , part fiction , tyehimba jesss much...   \n",
       "17  andrew barger , award-winning author engineer ...   \n",
       "18  libertarianism isnt winning elections ; first ...   \n",
       "19  wherever go , whatever , . . . dont anything s...   \n",
       "\n",
       "                                Description_processed  \n",
       "0   [hard, imagine, world, without, light, attic, ...  \n",
       "1   [erotic, absorb, write, starling, power, new, ...  \n",
       "2   [france, assez, proche, n√¥tre, homme, sengage,...  \n",
       "3   [wicked, hipbone, girl, across, heart, word, l...  \n",
       "4   [renowned, historian, come, groundbreaking, na...  \n",
       "5   [patient, monster, roams, hall, soothe, hill, ...  \n",
       "6   [draw, extensive, experience, evaluate, applic...  \n",
       "7   [heart, soul, karen, hicks, come, woman, make,...  \n",
       "8   [reader, laura, hillenbrands, seabiscuit, unbr...  \n",
       "9   [praise, aracelis, girmay, girmays, every, los...  \n",
       "10  [since, assault, miss, annette, chetwynd, plag...  \n",
       "11  [book, important, complete, collection, sonnet...  \n",
       "12  [aaron, ledbetter, future, plan, since, bear, ...  \n",
       "13  [scott, pilgrims, life, totally, sweet, he, ye...  \n",
       "14  [punk, raw, power, rejuvenate, rock, summer, m...  \n",
       "15  [story, musical, revolution, happen, right, no...  \n",
       "16  [part, fact, part, fiction, tyehimba, jesss, m...  \n",
       "17  [andrew, barger, author, engineer, extensively...  \n",
       "18  [libertarianism, isnt, win, election, first, f...  \n",
       "19  [wherever, go, whatever, dont, anything, stupi...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer_en = WordNetLemmatizer()\n",
    "stemmer_fr = SnowballStemmer('french')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokeniser\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Supprimer stopwords\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stopwords_combined]\n",
    "\n",
    "    # Pos tagging anglais (utile pour lemmatization anglaise)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    processed_tokens = []\n",
    "    for token, tag in pos_tags:\n",
    "        if token in stopwords_en:\n",
    "            continue\n",
    "        if token in stopwords_fr:\n",
    "            # pour le fran√ßais : stemmer simple (car pas de lemmatiseur fiable dans NLTK)\n",
    "            processed_tokens.append(stemmer_fr.stem(token))\n",
    "        else:\n",
    "            # anglais : lemmatisation avec POS\n",
    "            wn_tag = get_wordnet_pos(tag)\n",
    "            processed_tokens.append(lemmatizer_en.lemmatize(token, wn_tag))\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "# Appliquer sur la colonne\n",
    "df['Description_processed'] = df['Description'].apply(preprocess_text)\n",
    "\n",
    "df[['Description', 'Description_processed']].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5041e4d",
   "metadata": {},
   "source": [
    "# Extraction des caract√©ristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca24f7c",
   "metadata": {},
   "source": [
    "**1. Vectoriser le texte √† l‚Äôaide de TfidfVectorizer()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "227f0924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la matrice TF-IDF (Nombre de documents, Nombre de features) : (1000, 5000)\n",
      "Type de la matrice TF-IDF : <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Aper√ßu du DataFrame TF-IDF :\n",
      "   aaron  abandon  abbot  abby  abduction  abigail   ability  able  abound  \\\n",
      "0    0.0      0.0    0.0   0.0        0.0      0.0  0.000000   0.0     0.0   \n",
      "1    0.0      0.0    0.0   0.0        0.0      0.0  0.000000   0.0     0.0   \n",
      "2    0.0      0.0    0.0   0.0        0.0      0.0  0.000000   0.0     0.0   \n",
      "3    0.0      0.0    0.0   0.0        0.0      0.0  0.000000   0.0     0.0   \n",
      "4    0.0      0.0    0.0   0.0        0.0      0.0  0.069156   0.0     0.0   \n",
      "\n",
      "   abraham  ...  yuki  zeal  zero  zeus  zimbardo  zodiac  zombie  zone  \\\n",
      "0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   0.0   \n",
      "1      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   0.0   \n",
      "2      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   0.0   \n",
      "3      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   0.0   \n",
      "4      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   0.0   \n",
      "\n",
      "   zorin  zuko  \n",
      "0    0.0   0.0  \n",
      "1    0.0   0.0  \n",
      "2    0.0   0.0  \n",
      "3    0.0   0.0  \n",
      "4    0.0   0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n",
      "Vectorizer TF-IDF sauvegard√© sous 'models/tfidf_vectorizer.pkl'\n",
      "\n",
      "--- Vectorisation TF-IDF termin√©e ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Supposons que ta colonne pr√©-trait√©e s'appelle 'Description_processed'\n",
    "# Elle contient des listes de tokens, il faut les convertir en cha√Ænes de caract√®res\n",
    "df['Description_processed_text'] = df['Description_processed'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialiser le vectorizer TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Appliquer fit_transform sur les descriptions pr√©trait√©es (texte)\n",
    "X = tfidf_vectorizer.fit_transform(df['Description_processed_text'])\n",
    "\n",
    "# Obtenir les noms des caract√©ristiques (features)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convertir la matrice TF-IDF en DataFrame pour exploration si besoin\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "# Affichage des informations\n",
    "print(f\"Forme de la matrice TF-IDF (Nombre de documents, Nombre de features) : {X.shape}\")\n",
    "print(f\"Type de la matrice TF-IDF : {type(X)}\")\n",
    "print(\"Aper√ßu du DataFrame TF-IDF :\")\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Sauvegarder le vectorizer pour r√©utilisation future\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, 'models/tfidf_vectorizer.pkl')\n",
    "print(\"Vectorizer TF-IDF sauvegard√© sous 'models/tfidf_vectorizer.pkl'\")\n",
    "\n",
    "print(\"\\n--- Vectorisation TF-IDF termin√©e ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf132484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbot</th>\n",
       "      <th>abby</th>\n",
       "      <th>abduction</th>\n",
       "      <th>abigail</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>abraham</th>\n",
       "      <th>...</th>\n",
       "      <th>yuki</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zimbardo</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zorin</th>\n",
       "      <th>zuko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.520509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows √ó 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aaron  abandon  abbot  abby  abduction  abigail   ability  able  \\\n",
       "0   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "1   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "2   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "3   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "4   0.000000      0.0    0.0   0.0        0.0      0.0  0.069156   0.0   \n",
       "5   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "6   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "7   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "8   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "9   0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "10  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "11  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "12  0.520509      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "13  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "14  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "15  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "16  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "17  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "18  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "19  0.000000      0.0    0.0   0.0        0.0      0.0  0.000000   0.0   \n",
       "\n",
       "    abound  abraham  ...  yuki  zeal  zero  zeus  zimbardo  zodiac  zombie  \\\n",
       "0      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "1      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "2      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "3      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "4      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "5      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "6      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "7      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "8      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "9      0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "10     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "11     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "12     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "13     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "14     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "15     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "16     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "17     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "18     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "19     0.0      0.0  ...   0.0   0.0   0.0   0.0       0.0     0.0     0.0   \n",
       "\n",
       "    zone  zorin  zuko  \n",
       "0    0.0    0.0   0.0  \n",
       "1    0.0    0.0   0.0  \n",
       "2    0.0    0.0   0.0  \n",
       "3    0.0    0.0   0.0  \n",
       "4    0.0    0.0   0.0  \n",
       "5    0.0    0.0   0.0  \n",
       "6    0.0    0.0   0.0  \n",
       "7    0.0    0.0   0.0  \n",
       "8    0.0    0.0   0.0  \n",
       "9    0.0    0.0   0.0  \n",
       "10   0.0    0.0   0.0  \n",
       "11   0.0    0.0   0.0  \n",
       "12   0.0    0.0   0.0  \n",
       "13   0.0    0.0   0.0  \n",
       "14   0.0    0.0   0.0  \n",
       "15   0.0    0.0   0.0  \n",
       "16   0.0    0.0   0.0  \n",
       "17   0.0    0.0   0.0  \n",
       "18   0.0    0.0   0.0  \n",
       "19   0.0    0.0   0.0  \n",
       "\n",
       "[20 rows x 5000 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head(20).round(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dfef24",
   "metadata": {},
   "source": [
    " --- Vectorisation du texte avec TfidfVectorizer ---\n",
    "\n",
    "- Instancier le TfidfVectorizer\n",
    "- On utilise max_features pour limiter le vocabulaire et rendre le mod√®le plus g√©rable\n",
    "\n",
    "- Adapter (fit) le vectorizer aux donn√©es d'entra√Ænement et transformer le texte\n",
    "- 'final_text_for_vectorization' est la colonne qui contient le texte nettoy√©, tokenis√© et stemm√©\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c420f8a",
   "metadata": {},
   "source": [
    "- **Term Frequency (TF)** : Il s'agit simplement du nombre de fois qu'un mot appara√Æt dans un email. Un mot qui appara√Æt fr√©quemment dans un document a une valeur TF √©lev√©e.\n",
    "\n",
    "- **Inverse Document Frequency (IDF)** : Cette mesure √©value l'importance d'un mot dans l'ensemble du corpus d'emails. Un mot qui est tr√®s commun et appara√Æt dans de nombreux documents (comme \"le\", \"la\", \"un\", qui sont des stopwords) aura une valeur IDF faible, car il n'est pas tr√®s discriminant. √Ä l'inverse, un mot rare qui n'appara√Æt que dans un petit nombre d'emails (comme \"urgent\", \"gagner\" pour les spams) aura une valeur IDF √©lev√©e.\n",
    "\n",
    "- **Le score TF-IDF** est le produit de ces deux valeurs (TF√óIDF). Cela a pour effet de donner un poids √©lev√© aux mots qui sont fr√©quents dans un email sp√©cifique mais rares dans l'ensemble du corpus. Par cons√©quent, les mots pertinents pour la classification, tels que \"gratuit\" dans un spam, recevront un poids plus important que des mots g√©n√©riques comme \"bonjour\" ou \"email\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542d4d7",
   "metadata": {},
   "source": [
    " --- D√©finition de X (Caract√©ristiques) ---\n",
    "- X est la matrice TF-IDF r√©sultant de la vectorisation de votre texte pr√©trait√©.\n",
    "- Cette partie suppose que tfidf_vectorizer a d√©j√† √©t√© instanci√© et fit_transform√©\n",
    "- sur vos donn√©es, et potentiellement sauvegard√© et charg√© si vous travaillez dans une nouvelle session.\n",
    "- y est la colonne qui contient les √©tiquettes 'spam' ou 'ham'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
